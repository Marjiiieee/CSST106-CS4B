# -*- coding: utf-8 -*-
"""4B-CAYADONG-MP4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sGEvxeELX9DcOrshhgXMCBoNfKFy69HZ

# **CSST 106** - Perception and Computer Vision

**Name:** Cayadong, Marjelaine M.

**Program, Year & Section:** BSCS - 4B

**Installing OpenCV**
"""

!pip install opencv-python-headless

"""**Import necessary libraries**"""

import cv2
import matplotlib.pyplot as plt
import numpy as np
from skimage.feature import hog
from skimage import exposure

"""**Tasks**

***Task 1: Harris Corner Detection***
"""

# Task 1: Harris Corner Detection
def harris_corner_detection(image_path):
    img1 = cv2.imread('/content/niii.jpg')
    gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
    gray = np.float32(gray)

    # Harris Corner Detection
    corners = cv2.cornerHarris(gray, 2, 3, 0.04)
    corners = cv2.dilate(corners, None)
    img1[corners > 0.01 * corners.max()] = [0, 0, 255]

    # Display the original and corner-detected images
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.imshow(cv2.cvtColor(cv2.imread('/content/niii.jpg'), cv2.COLOR_BGR2RGB))
    plt.title('Original Image')
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))
    plt.title('Harris Corner Detection')
    plt.axis('off')

    plt.show()

"""***Task 2: HOG Feature Extraction***"""

# Task 2: HOG Feature Extraction
def hog_feature_extraction(image_path):
    img = cv2.imread('/content/niii.jpg')
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Extract HOG features with visualize=True to get the HOG image
    features, hog_image = hog(gray, pixels_per_cell=(8, 8), cells_per_block=(2, 2),
                              visualize=True, channel_axis=None)  # Note channel_axis=None for grayscale

    # Rescale HOG image intensity for better visualization
    hog_image = exposure.rescale_intensity(hog_image, in_range=(0, 10))

    # Display original image and HOG image
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.title('Original Image')
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.imshow(hog_image, cmap='gray')
    plt.title('HOG Features')
    plt.axis('off')
    plt.show()

"""***Task 3: ORB Feature Extraction and Matching***"""

# Task 3: ORB Feature Extraction and Matching
def orb_feature_matching(image_path1, image_path2):
    # Read images in grayscale
    img1 = cv2.imread('/content/lappy.jpg', cv2.IMREAD_GRAYSCALE)
    img2 = cv2.imread('/content/niii.jpg', cv2.IMREAD_GRAYSCALE)

    # ORB detector
    orb = cv2.ORB_create()
    keypoints1, descriptors1 = orb.detectAndCompute(img1, None)
    keypoints2, descriptors2 = orb.detectAndCompute(img2, None)

    # Check if descriptors are None
    if descriptors1 is None or descriptors2 is None:
        print("No descriptors found in one of the images. Cannot match features.")
        return

    # Ensure descriptors have the same type and dimensions
    if descriptors1.shape[1] != descriptors2.shape[1]:
        print("Descriptor dimensions do not match. Cannot perform matching.")
        return

    # BFMatcher with Hamming distance
    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
    matches = bf.match(descriptors1, descriptors2)
    matches = sorted(matches, key=lambda x: x.distance)

    # Draw matches
    img_matches = cv2.drawMatches(img1, keypoints1, img2, keypoints2, matches[:20], None, flags=2)

    # Display matching results
    plt.figure(figsize=(12, 6))
    plt.imshow(img_matches)
    plt.title('ORB Feature Matching')
    plt.axis('off')
    plt.show()

"""***Task 4: SIFT and SURF Feature Extraction***"""

# Task 4: SIFT and SURF Feature Extraction
def sift_and_surf_feature_extraction(image_path1, image_path2):
    img1 = cv2.imread('/content/niii.jpg')
    img2 = cv2.imread('/content/lappy.jpg')

    # SIFT
    sift = cv2.SIFT_create()
    keypoints1_sift, _ = sift.detectAndCompute(img1, None)
    keypoints2_sift, _ = sift.detectAndCompute(img2, None)
    img1_sift = cv2.drawKeypoints(img1, keypoints1_sift, None)
    img2_sift = cv2.drawKeypoints(img2, keypoints2_sift, None)

    # Display SIFT keypoints
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.imshow(cv2.cvtColor(img1_sift, cv2.COLOR_BGR2RGB))
    plt.title('SIFT Keypoints Image 1')
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.imshow(cv2.cvtColor(img2_sift, cv2.COLOR_BGR2RGB))
    plt.title('SIFT Keypoints Image 2')
    plt.axis('off')
    plt.show()

"""***Task 5: Feature Matching using Brute-Force Matcher***"""

# Task 5: Feature Matching using Brute-Force Matcher
def brute_force_feature_matching(image_path1, image_path2):
    img1 = cv2.imread('/content/niii.jpg', cv2.IMREAD_GRAYSCALE)
    img2 = cv2.imread('/content/lappy.jpg', cv2.IMREAD_GRAYSCALE)

    # ORB detector
    orb = cv2.ORB_create()
    keypoints1, descriptors1 = orb.detectAndCompute(img1, None)
    keypoints2, descriptors2 = orb.detectAndCompute(img2, None)

    # Brute-Force Matcher
    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
    matches = bf.match(descriptors1, descriptors2)
    matches = sorted(matches, key=lambda x: x.distance)

    # Draw matches
    img_matches = cv2.drawMatches(img1, keypoints1, img2, keypoints2, matches[:20], None, flags=2)

    # Display matching results
    plt.figure(figsize=(12, 6))
    plt.imshow(img_matches)
    plt.title('Brute-Force Feature Matching')
    plt.axis('off')
    plt.show()

"""***Task 6: Image Segmentation using Watershed Algorithm***"""

# Task 6: Image Segmentation using Watershed Algorithm
def watershed_segmentation(image_path):
    img = cv2.imread('/content/lappy.jpg')
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    # Noise removal
    kernel = np.ones((3, 3), np.uint8)
    opening = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=2)

    # Background and foreground separation
    sure_bg = cv2.dilate(opening, kernel, iterations=3)
    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)
    _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)

    # Unknown region
    sure_fg = np.uint8(sure_fg)
    unknown = cv2.subtract(sure_bg, sure_fg)

    # Marker labelling
    _, markers = cv2.connectedComponents(sure_fg)
    markers = markers + 1
    markers[unknown == 255] = 0

    # Watershed
    markers = cv2.watershed(img, markers)
    img[markers == -1] = [255, 0, 0]

    # Display segmented image
    plt.figure(figsize=(12, 6))
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.title('Watershed Segmentation')
    plt.axis('off')
    plt.show()

"""**Results**"""

# Call the functions after defining them
harris_corner_detection('/content/niii.jpg')
hog_feature_extraction('/content/niii.jpg')
orb_feature_matching('/content/niii.jpg', '/content/lappy.jpg')
sift_and_surf_feature_extraction('/content/niii.jpg', '/content/lappy.jpg')
brute_force_feature_matching('/content/niii.jpg', '/content/lappy.jpg')
watershed_segmentation('/content/lappy.jpg')